(Sec:BasisDim)=

# Basis and Dimension

## Introduction

The space $\R^n$ is sometimes called "$n$-dimensional space". What do people mean by that?
When, for instance, we have a plane in $\R^3$, and we look at it "from the inside" (like Flat-landers so to speak),
things just look as if we 'lived' in the plane $\R^2$.
If you want to know about Flat-landers:
<a href=https://en.wikisource.org/wiki/Flatland_(second_edition)>Flatland on Wikisource</a>, (or: ISBN:9789085711971).

Expressed more formally: a plane in $\R^3$ is a copy of the plane $\R^2$,
so it makes sense to say that such a plane has dimension 2. In this section we make this idea precise, making use of another
important concept related to subspaces, namely the concept of a basis.

(Subsec:BasisDim:Basis)=

## Basis of a Subspace

The set of vectors that generates a subspace is in no way unique as the following example shows:

::::::{prf:example}

Consider the four sets of vectors

$$
{\mathcal A}_1 = \left\lbrace
\begin{bmatrix}
1  \\ 0 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
1\\ 1 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
1\\ 1 \\ 1
\end{bmatrix}
\right\rbrace,
\quad{\mathcal A}_2 = \left\lbrace
\begin{bmatrix}
1  \\ 0 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
1\\ 1 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
1\\ 1 \\ 1
\end{bmatrix}
,
\begin{bmatrix}
1\\ 2 \\ 3
\end{bmatrix}
\right\rbrace
$$

and

$$
{\mathcal A}_3 = \left\lbrace
\begin{bmatrix}
1  \\ 2 \\ 3
\end{bmatrix}
,
\begin{bmatrix}
0\\ 1 \\ 2
\end{bmatrix}
,
\begin{bmatrix}
0\\ 0 \\ 1
\end{bmatrix}
\right\rbrace,
\quad{\mathcal A}_4 = \left\lbrace
\begin{bmatrix}
1  \\ 2 \\ 3
\end{bmatrix}
,
\begin{bmatrix}
0\\ 1 \\2
\end{bmatrix}
,
\begin{bmatrix}
0\\ 0 \\ 1
\end{bmatrix}
,
\begin{bmatrix}
1\\ 1 \\ 3
\end{bmatrix}
,
\begin{bmatrix}
2\\ 1 \\ 8
\end{bmatrix}
\right\rbrace.
$$

The subspaces generated by these vectors are equal:

$$
S  =  \Span{ {\mathcal A}_1}   =    \Span{ {\mathcal A}_2} =  \Span{ {\mathcal A}_3} =  \Span{ {\mathcal A}_4}
$$

namely, all four sets span the whole $\R^3$.

There is no a priori reason why ${\mathcal A}_1$ is a better set of generators than ${\mathcal A}_3$, but these two seem simpler than the two sets of four and even five vectors. These larger sets contain redundant vectors. They can be **thinned** to the three-vectors sets. By thinning we mean: vectors that are linear combinations of their predecessors can be deleted without reducing the span.
The sets ${\mathcal A}_1$ and ${\mathcal A}_3$ each consist of three linearly independent vectors. Because of that, further thinning would reduce the span. A linearly independent set of generators is in that sense a minimal set of generators, and deserves a special name. We call it a **basis**.

::::::

::::::{prf:definition}
:label: Dfn:BasisDim:Basis

A set of vectors ${\mathcal B} = \lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_r\rbrace$ is called a **basis** of a subspace $S$ if

<ol type = "i">

<li>

$S = \Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_r}$.

</li>

<li>

The set $\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_r\rbrace$ is linearly independent.

</li>
</ol>

::::::

::::::{prf:definition}
:label: Dfn:BasisDim:StandardBasis

The **standard basis** for $\R^n$ is given by the set

$$

{\mathcal E} = \lbrace\vect{e}_1, \vect{e}_2, \ldots, \vect{e}_n\rbrace


$$

where the vector $\vect{e}_j$ stands for the $j$-th column of the $n \times n$ identity matrix.

::::::

::::::{prf:remark}

This basis was already defined in {numref}`Sec:LinTrafo`, Equation {eq}`Eq:LinTrafo:StandardBasis`, along with the standard matrix of a linear transformation.

::::::

::::::{prf:example}
:label: Ex:BasisDim:StandardBasisR2R3

The standard basis in $\R^2$ can be visualized as two perpendicular unit vectors, e.g.
with $\vect{e}_1$ pointing to the right and $\vect{e}_2$ pointing upwards. Likewise we can depict the standard basis $\mathcal E$ of $\R^3$. See {numref}`Figure %s <Fig:BasisDim:StandardBasis>`.

```{applet}
:url: basisdim/standardbasis
:fig: Images/Fig-BasisDim-StandardBasis.svg
:name: Fig:BasisDim:StandardBasis
:class: dark-light

The standard bases in $\R^2$ and $\R^3$.
```

::::::

::::::{exercise}
:label: Exc:BasisDim:CheckStandardBasis

Check that the set

$$
{\mathcal E} =
\left\lbrace
\begin{bmatrix}
1 \\ 0 \\ 0 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
0 \\ 1 \\ 0 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
0 \\ 0 \\ 1 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 1
\end{bmatrix}
\right\rbrace.
$$

is indeed a basis for $\R^4$.

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:CheckStandardBasis`
:class: solution, dropdown

The set is linearly independent:

$$
 x_1\begin{bmatrix}
1 \\ 0 \\ 0 \\ 0
\end{bmatrix}
+x_2
\begin{bmatrix}
0 \\ 1 \\ 0 \\ 0
\end{bmatrix}
+x_3
\begin{bmatrix}
0 \\ 0 \\ 1 \\ 0
\end{bmatrix}
+x_4
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 1
\end{bmatrix} =
\begin{bmatrix}
x_1 \\x_2\\x_3\\x_4
\end{bmatrix} = \begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \quad \iff \quad \vect{x} = \begin{bmatrix}
x_1 \\x_2\\x_3\\x_4
\end{bmatrix} = \vect{0}.
$$

The set $\{\vect{e}_1, \vect{e}_2, \vect{e}_3, \vect{e}_4\}$ also spans the whole $\R^4$, as for any vector $\vect{x} = \begin{bmatrix}x_1 \\x_2\\x_3\\x_4 \end{bmatrix}$ <br>
it holds that
$\vect{x} = x_1\vect{e}_1 + x_2\vect{e}_2 + x_3\vect{e}_3 + x_4\vect{e}_4$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/7ecdf0d8-e529-4589-82f6-7207f229cd87?id=88187
:label: grasple_exercise_4_2_A
:dropdown:
:description: To check whether a set of three vectors forms a basis for $\R^3$.

::::::

Because of {prf:ref}`Thm:Subspaces:AllSubspacesRn`, which states that any subspace $S$ of $\R^n$ is of the form

$$
S = \Span{\vect{v}_1, \ldots, \vect{v}_r} \quad \text{with  }
\lbrace\vect{v}_1, \ldots, \vect{v}_r\rbrace \text{  linearly independent}
$$

a basis for a subspace always exists.

If $S$ is not the trivial subspace $\lbrace\vect{0}\rbrace$ a basis is not unique.
(For the trivial subspace the only basis is the empty set.)

::::::{prf:proposition} Thinning and extending
:label: Prop:BasisDim:Thinning

<ol type = "i">

<li>

Suppose $S = \Span{\mathcal A} = \Span{\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_{k}}$.

Then the set ${\mathcal A}$ can be thinned to a basis of $S$, i.e. there is a subset

<BR>

$$

{\mathcal B} \subseteq \mathcal{A}


$$

such that

<BR>

$$

{\mathcal B} \text{  is linearly independent   and} \quad \Span{\mathcal B} = S.


$$

</li>

<li>

If $S$ is a subspace in $\R^n$ then any linear independent set $\mathcal{A}$ of vectors in $S$ can be extended to a basis of $S$.

So, there exists a set $\mathcal{B}$ containing $\mathcal{A}$ which is a basis for $S$.

</li>
</ol>

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:BasisDim:Thinning`
:class: tudproof

The ideas are quite straightforward.

As for the thinning, first of all, if one of the vectors in $\mathcal A$ is the zero vector, then we can omit that vector.

For the remaining vectors, starting from the second vector we act as follows:

<ul>

<li>

if the vector is a linear combination of its predecessors: omit it,

</li>

<li>

if it is not: keep it,

</li>

<li>

go to the next vector.

</li>
</ul>

All along the way the span of the vectors remains intact,
so equal to $S$, while at the end the remaining generators will form a linearly independent set.

As for the other statement, for that we can use an argument as in {prf:ref}`Prop:Subspaces:AllSubspacesR3`.
As long as the span of the vectors is not the whole subspace we can add a linearly independent vector from $S$. Since in $\R^n$ there are at most $n$ linearly independent vectors, this process will end after at most $n$ steps.

::::::

::::::{prf:example}
:label: Ex:BasisDim:TwoByFour

Let us find a basis for the column space and the null space of the matrix

$$
A =
\begin{bmatrix}
1 &  -3 & 3 & 5 \\
2 &  -6 & 1 & 5
\end{bmatrix}
.
$$

The first and the third column

$$
\begin{bmatrix}
1  \\ 2
\end{bmatrix}
\quad \text{and} \quad
\begin{bmatrix}
3  \\ 1
\end{bmatrix}
$$

are obviously independent, and span all of $\R^2$. Since the columns of $A$ cannot span more than $\R^2$, we may conclude

$$
\Col{A} = \R^2
$$

and the two chosen columns give a possible basis (see {numref}`Figure %s <Fig:BasisDim:BasisColA>`).

This would also be the basis we get when we apply the thinning process of {prf:ref}`Prop:BasisDim:Thinning`: to find a basis for the column space we then start with the set of four columns and from left to right delete columns that are linear combinations of their predecessors. We have

$$
\vect{a}_2  = -3\vect{a}_1 \quad \text{and} \quad \vect{a}_4 \in \Span{\vect{a}_1, \vect{a}_3},
$$

since

$$
\Span{\vect{a}_1, \vect{a}_3} = \mathbb{R}^2.
$$

So we can thin the set of four columns as follows

$$
\Span{\vect{a}_1,\vect{a}_2,\vect{a}_3,\vect{a}_4} =
\Span{\vect{a}_1,\vect{a}_3,\vect{a}_4} =
\Span{\vect{a}_1,\vect{a}_3},
$$

where the two vectors $\vect{a}_1,\vect{a}_3$ are linearly independent.

Once it is known that $ \Col{A}= \R^2$, **any** set of two independent vectors (not necessarily columns of $A$) may be taken as a basis.

For the null space of $A$ we have to solve the linear system

$$
A\vect{x} = \vect{0}.
$$

Row reduction of the augmented matrix leads to

$$
\left[\begin{array}{cccc|c} 1 &  -3 & 3  &  5  & 0\\
2 &  -6 & 1  &  5 &  0
\end{array}
\right]
\sim
\left[\begin{array}{cccc|c}  1 &  -3 &  3 &  5  & 0\\
0 &   0 & -5 & -5 &  0
\end{array}
\right]
\sim
\left[\begin{array}{cccc|c}  1 &  -3 & 0  &  2 &  0\\
0 &   0 & 1  &  1 &  0
\end{array}
\right]
.
$$

We can read off the solutions:

$$
\begin{cases}
x_1 = 3x_2 - 2x_4 \\
x_2 = x_2 \\
x_3 = -x_4 \\
x_4= x_4
\end{cases}
\quad\Longrightarrow\quad\quad
\begin{bmatrix}
x_1\\x_2\\x_3\\x_4
\end{bmatrix}
=
c_1
\begin{bmatrix}
3\\1\\0\\0
\end{bmatrix}
+
c_2
\begin{bmatrix}
-2\\0\\-1\\1
\end{bmatrix}
$$

We see that the set

$$
\left\lbrace
\begin{bmatrix}
3\\1\\0\\0
\end{bmatrix}
,
\begin{bmatrix}
-2\\0\\-1\\1
\end{bmatrix}
\right\rbrace
$$

spans the null space. It is also linearly independent, so it is a basis.

::::{figure} Images/Fig-BasisDim-BasisColA.svg
:name: Fig:BasisDim:BasisColA
:class: dark-light

The basis $\lbrace\vect{a}_1,\vect{a}_3\rbrace$ of $\Col{A}$.
::::

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/6c836722-f664-434d-bf7d-3a4f86ff0187?id=88188
:label: grasple_exercise_4_2_B
:dropdown:
:description: To find a basis for span$\{\vect{b}_1, \ldots, \vect{b}_5\}$ by thinning.

::::::

The above example ({prf:ref}`Ex:BasisDim:TwoByFour`) lends itself to a few more observations that lead to a more efficient way to find bases for the column space and the null space of any matrix.

::::::{prf:remark}
:label: Rem:BasisDim:ColAndNulSpace

First of all: to find a basis for the null space, we have to solve

$$

A\vect{x} = \vect{0}.
\nonumber


$$

We typically do this by row reducing the augmented matrix

$$

[ A \,|\, \vect{0} ].
\nonumber


$$

However, there is no need to add the column of zeros, as this column remains zero throughout.
So we can just row reduce the matrix $A$.

More importantly, **row reduction does not change the linear relations between the columns**. Below we will make this precise. For now, look at the above example: all through the reduction process the second column remains equal to $(-3)$ times the first column.

Also, in the final (echelon) matrix

$$
[ E \,|\, \vect{0}]   =
\left[\begin{array}{cccc|c} 1 &  -3 & 0  &  2 & 0\\
0 &   0 & 1  &  1  & 0
\end{array}
\right]
=
\left[\begin{array}{cccc|c} \vect{e}_1 &  \vect{e}_2 & \vect{e}_3  &
\vect{e}_4 & \vect{0}
\end{array}
\right]
$$

the relation

$$

\vect{e}_4 = 2\vect{e}_1 + \vect{e}_3
\nonumber


$$

can be read off. The same relation holds between the columns of $A$:

$$
\vect{a}_4 =
\begin{bmatrix}
5  \\ 5
\end{bmatrix}
=
2
\begin{bmatrix}
1  \\ 2
\end{bmatrix}
+
\begin{bmatrix}
3  \\ 1
\end{bmatrix}
= 2\vect{a}_1 + \vect{a}_3,
$$

a relation that is easily overlooked at first sight.

Most importantly: independent columns of the echelon matrix $E$, i.e. columns of $E$ that are not linearly related, correspond to independent columns of $A$. The pivot columns of $E$ are obviously a maximal set of linearly independent columns of $E$, thus the corresponding columns of $A$ give a maximal set of independent columns of $A$, i.e. a basis of the column space. In the example these are the first and the third column of $A$.

::::::

The observations in the last remark are formalized in the following proposition.

::::::{prf:proposition}
:label: Prop:BasisDim:PivotColsBasis

Suppose $A$ is an $m\times n$ matrix, and $E$ is its reduced echelon form. Then we can deduce the following:

<ol type = "i">

<li>

The pivot columns of the matrix $A$ give a basis for the column space of $A$.

</li>

<li>

The null space of $A$ is equal to the null space of $E$.

</li>
</ol>

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:BasisDim:PivotColsBasis`
:class: tudproof

The main idea of the first statement: row operations do not alter the linear relations between the columns of a matrix. Now why is this so?
Let

$$

A =
\begin{bmatrix}
\vect{a}_1&\vect{a}_2&\ldots &\vect{a}_n
\end{bmatrix}
.
\nonumber


$$

We have seen before that a non-trivial linear relation

$$

c_1\vect{a}_1 + c_2\vect{a}_2+ \ldots + c_n\vect{a}_n =  \vect{0}
\nonumber


$$

between vectors $\vect{a}_1,\vect{a}_2,\ldots,\vect{a}_n$ corresponds to a non-trivial solution

$$

\vect{c} =
\begin{bmatrix}
c_1 \\ \vdots \\ c_n
\end{bmatrix}
\neq \vect{0}
\nonumber


$$

of the equation

$$

A\vect{x} = \vect{0}.
\nonumber


$$

Recall that row operations correspond to pre-multiplications with elementary matrices $E_j$

$$

A \quad \sim \quad E_1A \quad  \sim  \quad E_2E_1 A \quad  \sim \quad \ldots\quad  \sim  \quad (E_k\cdots E_1)A = E,
\nonumber


$$

where the last matrix $E$ is the reduced echelon form of the matrix $A$.

Since elementary matrices are invertible, and their product is as well, we have

$$

A\vect{c} = \vect{0}  \quad \iff \quad  (E_k\cdots E_1)A\vect{c} = \vect{0} \quad \iff \quad E\vect{c} = \vect{0}.
\nonumber


$$

This settles statement (ii).

We continue, to prove (i) as well.
We have just shown that the linear relations of the columns of $A$ are exactly the same as the linear relations of the columns of its equivalent reduced echelon form. In a reduced echelon matrix it is clear that the pivot columns give a maximal independent set of columns, i.e., a maximal set of columns for which there are no non-trivial linear relations. The same can then be said about the corresponding columns of $A$.

::::::

::::::{prf:example}
:label: Ex:BasisDim:FourByFour

We will find bases for the column space and the null space of the matrix

$$
A =
\begin{bmatrix}
1 & 2 & 4 & 2 \\
2 & 3 & 5 & 4 \\
4 & 2 &-2 & 6 \\
2 & 1 &-1 &7
\end{bmatrix}
.
$$

To this end we first row reduce the matrix:

$$
A \sim
\begin{bmatrix}
1 & 2 & 4 & 2 \\
0 &-1 &-3 & 0 \\
0 &-6 &-18&-2 \\
0 &-3 &-9 &3
\end{bmatrix}
\sim
\begin{bmatrix}
1 & 0 &-2 & 2 \\
0 &-1 &-3 & 0 \\
0 & 0 & 0 &-2 \\
0 & 0 & 0 & 3
\end{bmatrix}
\sim
\begin{bmatrix}
1 & 0 &-2 & 0 \\
0 & 1 & 3 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{bmatrix}
= E.
\nonumber
$$

From the reduced echelon matrix we read off that the pivots are in the first, second and fourth column. Note that these columns are indeed a basis for the column space of $E$.
Because of the previous proposition, the corresponding columns of $A$ give a (possible) basis for $\Col{A}$:

$$
{\mathcal B} = \left\lbrace
\begin{bmatrix}
1 \\   2 \\  4  \\  2
\end{bmatrix}
,
\begin{bmatrix}
2 \\3 \\ 2 \\ 1
\end{bmatrix}
,
\begin{bmatrix}
2 \\  4 \\ 6 \\ 7
\end{bmatrix}
\right\rbrace.
$$

The solution of the equation $A\vect{x} = \vect{0}$ can also be easily read off from the reduced echelon form:

$$

\Nul{A}=\Nul{E}=
\Span{
\begin{bmatrix}
2 \\   -3 \\  1  \\  0
\end{bmatrix}
}.
\nonumber


$$

Note that the number of vectors in this basis for $\Col{A}$ is equal to the number of pivots, and the number of vectors in this basis for the null space is equal to the number of columns without a pivot.

::::::

::::::{exercise}
:label: Exc:BasisDim:TF-ColA-ColE

Is the following statement true or false:

if the matrix $E$ is the reduced echelon form of the matrix $A$, then

$$
\Col{A} = \Col{E}.
$$

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:TF-ColA-ColE`
:class: solution, dropdown

The statement is _false_. <BR>
For instance, look at {prf:ref}`Ex:BasisDim:FourByFour`. In that example all vectors in $\Col{(E)}$ have a zero on position four, and there are (many) vectors in $\Col{(A)}$ that have a nonzero fourth entry. So definitely $\Col{(A)} \neq \Col{(E)}$.

::::::

To find a basis of the column space we only have to look at the pivot columns of _any_ (row) equivalent echelon matrix. The _corresponding_ columns of $A$ provide a basis for the column space of $A$. For a basis of the null space it is preferable to work with the equivalent _reduced_ echelon matrix.

To find a basis of the column space by taking the pivot columns is more efficient than do the thinning step-by-step. By omitting the non-pivot columns the thinning is done at one stroke.

::::::{prf:example}
:label: Ex:BasisDim:BasesColAndNulSpace4by5

It is given that matrix

$$
A =
\begin{bmatrix}
1  &  1 & 2 &  1 & -1 \\
2  &  2 & 7 &  0 &  3 \\
1  &  1 & 5 &  1 & 5 \\
3  &  3 & 9 & -1 & 1
\end{bmatrix}
$$

is equivalent to the echelon matrix

$$
E =
\begin{bmatrix}
1  &  1 & 2 &  1 & -1 \\
0  &  0 & 3 & -2 & 5 \\
0  &  0 & 0 &  2 & 1 \\
0  &  0 & 0 &  0 & 0
\end{bmatrix}.
$$

The pivots are in the first, the third and the fourth column, so the corresponding columns of $A$ give a basis for $\Col{A}$:

$$
{\mathcal B} = \left\lbrace
\begin{bmatrix}
1   \\ 2  \\1 \\ 3
\end{bmatrix}
,
\begin{bmatrix}
2   \\ 7  \\5 \\ 9
\end{bmatrix}
,
\begin{bmatrix}
1   \\ 0  \\1 \\-1
\end{bmatrix}
\right\rbrace.
$$

For a basis of $\Nul{A}$ we need more work.

::::::

::::::{exercise}
:label: Exc:BasisDim:FinishExampleNulspace

Find a basis for the null space of the matrix in the previous example.

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:FinishExampleNulspace`
:class: solution, dropdown

We row reduce the matrix $E = \begin{bmatrix}
1  &  1 & 2 &  1 & -1 \\
0  &  0 & 3 & -2 & 5 \\
0  &  0 & 0 &  2 & 1 \\
0  &  0 & 0 &  0 & 0
\end{bmatrix}$ further to row reduced echelon form.

A short computation yields

$$
 E = \begin{bmatrix}
1  &  1 & 2 &  1 & -1 \\
0  &  0 & 3 & -2 & 5 \\
0  &  0 & 0 &  2 & 1 \\
0  &  0 & 0 &  0 & 0
\end{bmatrix} \quad \sim \quad  E_2 = \begin{bmatrix}
1  &  1 & 0 &  1 & -5.5 \\
0  &  0 & 1 &  0 & 2 \\
0  &  0 & 0 &  1 & 0.5 \\
0  &  0 & 0 &  0 & 0
\end{bmatrix}
$$

We can read off that a basis for $\Col{(E)} = \Col{(A)}$ is given by

$$
  \mathcal{B} = \left\{\begin{bmatrix} -1 \\1\\0\\0\\0\end{bmatrix}, \,
   \begin{bmatrix} 11/2\\0 \\-2\\-1/2\\1 \end{bmatrix} \right\}.
$$

::::::

A non-trivial subspace of $\R^n$ has infinitely many bases. They do share one feature, which is captured in the following theorem.

::::::{prf:theorem}
:label: Thm:BasisDim:EqualDim

Every basis of a fixed subspace $S$ in $\R^n$ has the same number of elements.

::::::



::::::{admonition} Proof of&nbsp;{prf:ref}`Thm:BasisDim:EqualDim`
:class: tudproof

The proof is an immediate consequence of {prf:ref}`Thm:LinInd:TooManyVectsimpliesLinDep`
from the section on linear independence. Because of its vital important we restate it here (check the side note).

We can use this as follows.

Suppose

$$
\lbrace\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_k \rbrace \quad \text{and}
\quad \lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell} \rbrace
$$

are two bases for the same subspace $S$.

So, they are both independent sets, and

$$

S = \Span{\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_k } =
\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell}}.
$$

Since

$$

\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell}} \subseteq \Span{\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_k},


$$

the assumption

$$

\ell > k
\nonumber


$$

would imply

$$

\text{the set  } \lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell} \rbrace
\text{  is linearly dependent.}
\nonumber


$$

So $\ell > k$ leads to a contradiction.

Reversing the two sets we see that $k > \ell$ is also impossible, and we can conclude that we must have

$$

k = \ell.
\nonumber


$$

::::::

::::::{prf:example}
:label: Ex:PlaneWithBases

Let $\mathcal{P}$ be the plane in $\R^3$ described by the equation

$$

2x_1 + x_2 - 6x_3 = 0.
\nonumber


$$

Since the equation is homogeneous, $\mathcal{P}$ is a subspace in $\R^3$.

We could also say

$$
{\mathcal P} = \Nul{A}, \quad \text{where} \quad A =
\begin{bmatrix}
2 & 1 & -6
\end{bmatrix}
.
$$

By taking $x_2$ and $x_3$ as free variables we find that the {index}`general solution` of this linear system (of one equation in three unknowns) is given by

$$
\left\lbrace
\begin{array}{ll}
x_1   = &  -\tfrac12x_2 + 3x_3\\
x_2   = & x_2\\
x_3   = & x_3
\end{array}
\right.\Longrightarrow
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}
=
c_1
\begin{bmatrix}
-\nicefrac{1}{2} \\ 1 \\ 0
\end{bmatrix}
+
c_2
\begin{bmatrix}
3 \\ 0 \\ 1
\end{bmatrix}
= c_1\vect{u}_1 + c_2\vect{u}_2.
$$

Thus $\{\vect{u}_1, \vect{u}_2\}$ is a basis for $\mathcal{P}$.

We can also take $x_1$ and $x_3$ as free variables, and then find

$$
{\mathcal P} = \Span{
\begin{bmatrix}
1   \\ -2 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
0 \\ 6\\1
\end{bmatrix}
},
$$

which provides an alternative basis for $\mathcal{P}$.

::::::

::::::{exercise}
:label: Exc:BasisDim:OtherBasesForPlane

<ol type = "i">

<li>

Give another basis of the plane $\mathcal P$, which does not contain a vector that is a multiple of one of the vectors $\begin{bmatrix} 1   \\ -2 \\ 0 \end{bmatrix}$, $\begin{bmatrix}3   \\ 0 \\ 1\end{bmatrix}$ and $\begin{bmatrix}0 \\ 6\\1\end{bmatrix}$.

</li>

<li>

Show that **any** set of two independent vectors in the plane $\mathcal P$ generates the plane, hence forms a basis.

</li>
</ol>

::::::

(Subsec:BasisDim:Dimension)=

## Dimension of a Subspace

::::::{prf:definition} Dimension

The **dimension** of a subspace $S$ is the number of elements in a
(i.e., any) basis for $S$. &nbsp; Notation: &nbsp; dim $S$.

::::::

Because of {prf:ref}`Thm:BasisDim:EqualDim` this is a good definition.

::::::{prf:example}
:label: Ex:BasisDim:BasesTrivialSubspaces

For the trivial subspace $S = \lbrace\vect{0}\rbrace$ we postulated that its basis is the empty set. Thus,

$$
\text{dim}\, S = \text{dim}\lbrace\vect{0}\rbrace = 0.
$$

For the other trivial subspace, the whole $\R^n$, the standard basis

$$
{\mathcal E} = \lbrace\vect{e}_1, \vect{e}_2, \ldots, \vect{e}_n\rbrace,
$$

has exactly $n$ elements, So

$$
\text{dim} \,\R^n = n.
$$

::::::

You can look upon the dimension of a subspace as a means to describe its size. This is not to be confused with the size of a vector which is just the number of entries of the vector.

::::::{exercise}
:label: Exc:BasisDim:TF:DimOfVector

Is the following statement true or false?

The dimension of the vector $\vect{u} = \begin{bmatrix} 3\\4 \end{bmatrix} $ is 2.

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:TF:DimOfVector`
:class: solution, dropdown

The statement is _false_. The attribute dimension is only defined for _subspaces_.

A correct statement would be:
the _size_ of the vector $\vect{u} = \begin{bmatrix} 3\\4 \end{bmatrix} $ is equal to 2.

::::::

The following proposition sometimes helps to show that a set of vectors is a basis of a subspace $S$. We can use it to establish in an indirect way that a set of vectors generates the (whole) subspace.

::::::{prf:proposition}
:label: Prop:BasisDim:TwoOfThreeSuffice

For a set of vectors $ \lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell} \rbrace $ in a subspace $S$ _of dimension $k$_ each pair of the following three properties implies the remaining property.

<ol type = "i">

<li>

$\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell} \rbrace$ is linearly independent;

</li>

<li>

$\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell}}=S$;

</li>

<li>

$\ell = k$.

</li>
</ol>

Another way to put it: once it is known that the dimension of $S$ equals $k$, each set that satisfies two of the three properties is a basis for $S$.

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:BasisDim:TwoOfThreeSuffice`
:class: tudproof

We first show the "easy" part:

$$
\text{i.   and  ii.} \quad \Longrightarrow \quad   \text{iii.}
$$

Well, if $\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_{\ell} \rbrace$ is linearly independent and spans $S$, then it is a basis for $S$. Since all bases of $S$ contain the same number $k = $ dim $S$ vectors, it follows that $\ell = k$.

Next, let us prove

$$
\text{i.   and  iii.} \quad \Longrightarrow \quad   \text{ii.}
$$

So suppose $\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k \rbrace$ is linearly independent, and is contained in $S$. We have to show that

$$
\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k } = S,
$$

i.e., each vector $\vect{s}$ in $S$ is contained in
$\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k }$. <BR>
To show this, let $\vect{s}$ be an arbitrary vector in $S$.  
Furthermore, let

$$
\lbrace\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_{k} \rbrace
$$

be any basis of $S$.
Then the set

$$
\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k,\vect{s}\rbrace
$$

which contains $k+1$ elements, is contained in the set

$$
S = \Span{\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_k}.
$$

So

$$
\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k, \vect{s}\rbrace \subseteq \Span{\vect{a}_1, \vect{a}_2, \ldots, \vect{a}_k}.
$$

Invoking {prf:ref}`Thm:LinInd:TooManyVectsimpliesLinDep` we get that
$\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k, \vect{s} \rbrace$
is linearly dependent.

Since we assumed $\lbrace\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k\rbrace$ is linearly independent, we may conclude that

$$
\vect{s} \in \Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k}.
$$

Thus we have shown that $\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k }$ indeed contains each vector in $S$, i.e.

$$
\Span{\vect{b}_1, \vect{b}_2, \ldots, \vect{b}_k } = S.
$$

The remaining part we leave to the reader.

::::::

::::::{exercise}
:label: Exc:BasisDim:ProofOf2OutOf3

In {prf:ref}`Prop:BasisDim:TwoOfThreeSuffice` prove

$$
\text{ii.   and   iii.}  \quad  \Longrightarrow \quad     \text{i.}
$$

::::::

::::::{prf:remark}

Take another look at {prf:ref}`Ex:PlaneWithBases`. We constructed a basis for the plane $\mathcal P$ given by the equation

$$
2x_1 + x_2 - 6x_3 = 0.
$$

This basis contained two vectors, so

$$
\text{dim}{\mathcal P} = 2.
$$

From the {prf:ref}`Prop:BasisDim:TwoOfThreeSuffice` it follows that _any_ set of two linearly independent vectors in $\mathcal{P}$ is a basis for $\mathcal{P}$.

::::::

::::::{prf:example}

Any set $S$ of $n$ independent vectors in $\R^n$ spans $\R^n$.

Namely we know that $\R^n$ has dimension $n$, so such a set $S$ satisfies i. and iii. of {prf:ref}`Prop:BasisDim:TwoOfThreeSuffice`.

Of course this statement can also be proved by an argument involving pivots.

::::::

We conclude this subsection with a theorem that relates the column space and the null space of an $m\times n$ matrix $A$. At first sight there does not seem much to relate: the column space is a subspace of $\R^m$, whereas the null space 'lives' in $\R^n$. However, the null space, i.e. the solution set of the equation

$$
A\vect{x} = \vect{0},
$$

does say something about the linear relations between the columns of $A$.

$$
[ \,\vect{a}_1 \,\, \vect{a}_2 \,\, \ldots \,\, \vect{a}_n\, ]\vect{x} = \vect{0} \quad \iff \quad
x_1\vect{a}_1+ x_2\vect{a}_2+  \ldots +x_n\vect{a}_n = \vect{0}.
$$

The more linear relations there are, i.e., the larger $\Nul{A}$ is,
the smaller will be $\Span{\vect{a}_1,  \vect{a}_2,  \ldots,  \vect{a}_n}$, i.e. $\Col{A}$.

::::::{prf:theorem} Dimension Theorem
:label: Thm:BasisDim:DimensionTheorem

For any $m\times n$ matrix $A$:

$$
\text{dim }\Col{A}  + \text{dim }\Nul{A} = n.
$$

::::::


::::::{admonition} Proof of&nbsp;{prf:ref}`Thm:BasisDim:DimensionTheorem`
:class: tudproof

The proof consists of combining several properties of this section.

We have seen ({prf:ref}`Prop:BasisDim:PivotColsBasis`) that the dimension of the column space is equal to the number of pivots. Say, this number is given by $p$.

Likewise, the dimension of the null space is equal to the number of free variables in the solution of

$$
A\vect{x} = \vect{0},
$$

and that is exactly the number of non-pivot columns, which is $n-p$.

We conclude:

$$
\text{dim Col }A + \text{dim Nul }A = p + (n-p) = n.
$$

::::::

::::::{prf:example}
:label: Ex:BasisDim:DimColAndNulSpace3by5

If $A$ is a $3\times5$ matrix, the dimension of the null space of $A$ must be at least $2$.

Namely,

$$
\text{Col}\,A \subseteq \R^3 \quad \text{implies} \quad \text{dim Col}\, A \leq 3.
$$

From

$$
\text{dim } \Col{A} +  \text{dim } \Nul{A} = 5,
$$

it then follows that

$$
\text{dim }\Nul{A} = 5 - \text{dim }\Col{A}   \geq   5 - 3 = 2.
$$

::::::

(Subsec:BasisDim:RowspaceRank)=

## Row Space and Rank

There is a third subspace connected to an $m\times n$ matrix $A$, namely, the subspace generated by the rows. This is a subspace of $\R^n$, and it may come as a small surprise that it has the same dimension as the column space of $A$, which is a subspace of $\R^m$. This dimension we will call the **rank** of a matrix.

Since we are used to write vectors in $\R^n$ as column vectors, we define the row space of a matrix as follows:

::::::{prf:definition}
:label: Dfn:Basisdim:RowSpace

The **row space** of a matrix $A$ is defined as the column space of its transpose:

$$
\text{Row}\,{A} = \Col{A^T}.
$$

::::::

::::::{prf:example}
:label: Ex:BasisDim:Rowspace2by4

For the matrix

$$
A =
\begin{bmatrix}
1 &  -3 & 3 & 5 \\
2 &  -6 & 1 & 5
\end{bmatrix}
$$

the rows are obviously linearly independent,
as are the columns of

$$
A^T =
\begin{bmatrix}
1 & 2 \\   -3 & -6 \\ 3 & 1 \\ 5 & 5 \\
\end{bmatrix}.
$$

So we find

$$
\Row{A} =\Span{
\begin{bmatrix}
1 \\  -3 \\ 3 \\ 5
\end{bmatrix}
,
\begin{bmatrix}
2 \\  -6 \\  1 \\ 5
\end{bmatrix}
},
$$

thus $\Row{A}$ is a subspace of $\R^4$ of dimension 2.

::::::

This is the same matrix $A$ as in {prf:ref}`Ex:BasisDim:TwoByFour`, where we found that the column space is the whole $\R^2$, which gives a first instance of the property

$$
\text{dim } \Row{A} = \text{dim } \Col{A},
$$

a relation that will appear to hold for every $m \times n$ matrix ({prf:ref}`Prop:BasisDim:EqualDimRowColSpace`).

We will work towards that proposition in two steps.
First we consider another example, where the matrix is an echelon matrix.

::::::{prf:example}
:label: Ex:RowSpaceEchelonMatrix

Consider the $4 \times 5$ matrix

$$
M =
\begin{bmatrix}
1 & 0 & 4 & 6 & 3 \\
0 & 2 & 3 & 1 & 5 \\
0 & 0 & 0 & 4 & 7 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}.
$$

The matrix is in echelon form, so the three pivot columns give a basis for the column space. Thus

$$
\text{dim } \Col{M} = 3.
$$

The nonzero columns of

$$
M^T =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 \\
4 & 3 & 0 & 0 \\
6 & 1 & 4 & 0 \\
3 & 5 & 7 & 0
\end{bmatrix}
,
$$

which are in a one-one-correspondence with the first three rows of $M$,
give a basis for the row space of $M$, whence

$$
\text{dim } \Row{M} = 3.
$$

::::::

::::::{prf:proposition}
:label: Prop:BasisDim:EquivalentMatricesEqualRowspaces

If $A$ and $B$ are equivalent matrices, then

$$
\Row{A} =  \Row{B}.
$$

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:BasisDim:EquivalentMatricesEqualRowspaces`
:class: tudproof

Recall that equivalence here means that by row operations we can transform $A$ into $B$ (and vice versa). So we have to show that row operations do not change the row space.

If $B$ is the result of applying row operations to $A$, each row of $B$ will be a linear combination of the rows of $A$. Thus it follows that the row space of $B$ is contained in the row space in $A$. In short,

$$
\Row{B} \subseteq  \Row{A}.
$$

Since the equivalence works both ways we can interchange $A$ and $B$ and conclude that also

$$
\Row{A}  \subseteq  \Row{B},
$$

and if two sets are contained in each other they must of course be equal.

::::::

::::::{prf:proposition}
:label: Prop:BasisDim:EqualDimRowColSpace

For each matrix $A$ the row space and the column space have the same dimension.

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:BasisDim:EqualDimRowColSpace`
:class: tudproof

If a matrix $A$ is row reduced to an echelon matrix $E$ we know that

$$
\Row{A} =  \Row{E}.
$$

For an echelon matrix it is very easy to find a basis of the row space. As in {prf:ref}`Ex:RowSpaceEchelonMatrix`, we can just take the nonzero rows (written as columns).
The nonzero rows are the rows that start with a pivot, so we find

$$
\text{dim }\Row{A} = \text{dim }\Row{E}= \text{ number of pivots } = \text{dim } \Col{A},
$$

where at the last step we used {prf:ref}`Prop:BasisDim:PivotColsBasis`.

::::::

::::::{prf:example}
:label: Ex:BasisDim:Rowspace4x3

Consider the matrix

$$
A =
\begin{bmatrix}
1 & 2 & 4 & 2 \\
2 & 3 & 5 & 4 \\
4 & 2 &-2 & 6 \\
2 & 1 &-1 &7
\end{bmatrix}
$$

of {prf:ref}`Ex:BasisDim:FourByFour`.
In this example we have seen that

$$
A   \sim
\begin{bmatrix}
1 & 0 &-2 & 0 \\
0 & 1 & 3 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{bmatrix}
=   E.
$$

The first three rows of $E$ are obviously linearly independent, and so are the first three columns of $E^T$.
A possible basis for the row space of $A$:

$$
\Row{A} = \Row{E} = \Span{
\begin{bmatrix}
1 \\ 0 \\-2 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
0 \\ 1 \\ 3 \\ 0
\end{bmatrix}
,
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 1
\end{bmatrix}
}.
$$

Note that for a basis of the row space we don't have to go "back" from the rows of the echelon matrix to the corresponding rows of $A$. As a matter of fact,
this may lead to a wrong conclusion, as there is no one-to-one correspondence between the rows of $A$ and the rows of $E$. The reason is that during the row reduction process rows may have been swapped.

::::::

::::::{prf:example}

As an illustration of the last remark, consider the matrix

$$
A =
\begin{bmatrix}
1 &1 & 1 \\ 2&2&2 \\ 3&3&4
\end{bmatrix}
.
$$

Its reduced echelon form is

$$
E =
\begin{bmatrix}
1 &1 & 1 \\ 0&0&1 \\ 0&0&0
\end{bmatrix}
,
$$

so as a basis for the row space we can take

$$
\left\lbrace
\begin{bmatrix}
1 \\1\\1
\end{bmatrix}
,
\begin{bmatrix}
0 \\0\\1
\end{bmatrix}
\right\rbrace,
$$

corresponding to the first two rows of $E$. Taking the first two rows of $A$ (written as columns) in this case leads to the wrong answer.

::::::

::::::{exercise}
:label: Exc:BasisDim:ExpressInRowsOfE

In the above {prf:ref}`Ex:BasisDim:Rowspace4x3` find out how the four rows of the original matrix can be written as linear combinations of the rows of the reduced echelon form.

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:ExpressInRowsOfE`
:class: solution, dropdown

It can be done 'by inspection'

$$
   \begin{array}{ccl}
      [1, 1, 1] &=& 1\cdot [1,1,1] \,\, (+0\cdot [0, 0, 1] + 0\cdot [0,0,0])\\
      [2, 2, 2] &=& 2\cdot [1,1,1]  \\
      [3, 3, 4] &=& 3\cdot [1,1,1] + 1\cdot [0,0,1]
   \end{array}
$$

::::::

For future reference this seems to be **the** place for yet another definition:

::::::{prf:definition}

The **rank** of a matrix is defined as the dimension of its column space (or, for that matter, its row space):

$$
\text{rank} A = \text{dim }\Col{A}.
$$

::::::

::::::{prf:proposition}
:label: Prop:BasisDim:RankEqualsNrOfPivots

The rank of a matrix $A$ is equal to the number of pivots.

::::::

Namely, by {prf:ref}`Prop:BasisDim:PivotColsBasis` the dimension of the column space of a matrix is equal to the number of pivots.

::::::{exercise}
:label: Exc:BasisDim:TF:RankAEqualsRankAT

Prove the identity

$$
\text{rank}\left(A^T\right)
 = \text{rank}(A).
$$

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:TF:RankAEqualsRankAT`
:class: solution, dropdown

The rank of $A^T$ is the dimension of the column space of $A^T$, which is the dimension of the row space of $A$. <BR>
The rank of $A$ is the dimension of the column space of $A$.
<br/>
By {prf:ref}`Prop:BasisDim:EqualDimRowColSpace` dim Row$(A) = $ dim Row$(A)$.

::::::

The last theorem contains two reformulations of old material.

::::::{prf:theorem} Rank Theorem
:label: Thm:BasisDim:RankThm

<ol type = "i">

<li>

For each $m\times n$ matrix $A$:

$$
\text{rank}\,A = n -  \text{dim }\Nul{A}
$$

</li>

<li>

For each $n\times n$ matrix $A$:

$$
A \text{  is invertible  } \iff \text{ rank}\, A = n.
$$

</li>
</ol>

::::::

::::::{exercise}
:label: Exc:BasisDim:FindRelevantProps

The statements in {prf:ref}`Thm:BasisDim:RankThm` are both reformulations of earlier propositions (or theorems). Find these earlier propositions.

::::::

::::::{exercise}
:label: Exc:BasisDim:RankABLeqRankA

Suppose that $A$ and $B$ are matrices for which the product $AB$ is defined. Show that

$$
\text{rank}(AB) \leq \text{rank}\,A.
$$

::::::

::::::{admonition} Solution to&nbsp;{numref}`Exc:BasisDim:RankABLeqRankA`
:class: solution, dropdown

The statement in {numref}`Exc:Subspaces:ColABinColA` that says $\Col{AB} \subseteq \Col{A}$ immediately gives that

$$
\text{rank}(AB) = \text{dim Col}(AB) \leq \text{dim Col}(A) =\text{rank}\,A.
$$

::::::

::::::{exercise}
:label: Exc:BasisDim:RankABLeqRankB

Suppose that $A$ and $B$ are matrices for which the product $AB$ is defined. Show that

$$
\text{rank}(AB) \leq \text{rank}\,B.
$$

::::::

The following proposition combines the results of the last two exercises.

::::::{prf:proposition}
:label: Prop:BasisDim:RankAPEqualToRankPA

Suppose that $A$ and $P$ are $n\times n$ matrices and $P$ is _invertible_. Then

$$
\text{rank}(AP) =   \text{rank}(A) =  \text{rank}(PA).
$$

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:BasisDim:RankAPEqualToRankPA`
:class: tudproof

Suppose $A$ and $P$ are as stated. Then by {numref}`Exc:BasisDim:RankABLeqRankA`

$$
\text{rank}(AP) \leq \text{rank}(A) = \text{rank}(APP^{-1}) \leq \text{rank}(AP)
$$

The first inequality is the same as the inequality in {numref}`Exc:BasisDim:RankABLeqRankA`
and the second inequality follows from the same exercise by taking $A = AP$ and $B = P^{-1}$.
This proves $\text{rank}(AP) =   \text{rank}(A)$. <BR>
The identity $\text{rank}(A) =  \text{rank}(PA)$ follows by considering the transpose:

$$
\text{rank}(PA) = \text{rank}\left((PA)^T\right)
 = \text{rank}\left(A^TP^T\right).
$$

Here we have a product $A^TP^T$ with $P^T$ invertible, so we can use use the identities that we already proved, and find

$$
\text{rank}\left(A^TP^T\right)
 = \text{rank}\left(A^T\right)
 = \text{rank}(A).
$$

::::::

::::::{exercise}
:label: Exc:BasisDim:AB=0ImpliesSumRanksLeqN

Suppose that $A$ and $B$ are $n\times n$ matrices for which

$$
AB = O.
$$

Show that

$$
\text{rank}\, A + \text{rank}\, B \leq n.
$$

Does the conclusion also hold when $A$ is an $m\times n$ matrix and
$B$ is an $n\times m$ matrix?

::::::

## Grasple Exercises

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/9387d1c3-03b4-41ff-b7b6-bb0d0ee3c771?id=70632
:label: grasple_exercise_4_2_1
:dropdown:
:description: To check whether three vectors constitute a basis for $\R^3$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/7bf4ef59-9f95-4697-9309-f06078856988?id=70633
:label: grasple_exercise_4_2_2
:dropdown:
:description: To check whether three vectors constitute a basis for $\R^3$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/2ea2e118-a471-4688-9d89-87cd49cfddc2?id=70634
:label: grasple_exercise_4_2_3
:dropdown:
:description: To check whether a set of vectors forms a basis for $\R^3$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/1c00677f-eadb-4a13-84aa-5c7b07774f21?id=88189
:label: grasple_exercise_4_2_4
:dropdown:
:description: To find a basis for span$\{\vect{b}_1, \ldots, \vect{b}_5\}$ by thinning.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/ae1a690e-1e4d-40be-94d7-cf91121134f0?id=70649
:label: grasple_exercise_4_2_5
:dropdown:
:description: Finding bases for Col$(A)$ and Nul$(A)$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/9440dca7-7d2d-4f74-8679-c966be28d73f?id=70638
:label: grasple_exercise_4_2_6
:dropdown:
:description: Finding bases for Col$(A)$ and Nul$(A)$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/3d5483ab-4a1f-4caf-b979-9a4518551416?id=70640
:label: grasple_exercise_4_2_7
:dropdown:
:description: Finding bases for Col$(A)$ and Nul$(A)$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/b85a9ef2-3b82-47d9-89f3-212a435b8be2?id=70642
:label: grasple_exercise_4_2_8
:dropdown:
:description: To find a basis for a subspace of $\R^4$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/04d804ff-16ea-44ee-ac76-452a73a88859?id=70653
:label: grasple_exercise_4_2_9
:dropdown:
:description: To find a basis and the dimension of span$\{\vect{v}_1, ... , \vect{v}_4\}$ &nbsp; (in $\R^4$).

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/789d912e-6ac2-4f48-b92b-11ab33d5c949?id=70655
:label: grasple_exercise_4_2_10
:dropdown:
:description: To find rank$(A)$ and dim Nul$(A)$ for a given matrix.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/610e51bc-c62f-4da2-81f3-eeef106bba84?id=83409
:label: grasple_exercise_4_2_11
:dropdown:
:description: Find rank$(A)$ for a matrix $A$ containing a parameter $h$.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/98aae0f1-3cd9-43d8-aa9f-7c884f2c9527?id=83411
:label: grasple_exercise_4_2_12
:dropdown:
:description: Find rank$(A)$ for a matrix $A$ containing a parameter $h$.

::::::

The exercises below are more theoretical.

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/9dc4b61f-1ded-496c-82e1-8413bc0fa5e7?id=70644
:label: grasple_exercise_4_2_13
:dropdown:
:description: Can a subspace in $\R^3$ have a basis consisting of four vectors?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/dc1ffd49-1c32-407a-9042-f386b85c771d?id=70645
:label: grasple_exercise_4_2_14
:dropdown:
:description: Can a basis contain the zero vector?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/3576e4c9-2084-4e3f-af61-08f09d63ad82?id=70646
:label: grasple_exercise_4_2_15
:dropdown:
:description: Which of four statements about Col$(A)$ is correct?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/37a433f6-b15f-4b8a-8232-4098fe82e6c9?id=70647
:label: grasple_exercise_4_2_16
:dropdown:
:description: Which of five statements about Col$(A)$ is incorrect?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/2940572e-f3bf-40e9-bf01-c2244c6f9aa5?id=70648
:label: grasple_exercise_4_2_17
:dropdown:
:description: Which of four statements about Nul$(A)$ is incorrect?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/2f4a3540-9454-4b49-ac4c-0aae22fd5b50?id=70657
:label: grasple_exercise_4_2_18
:dropdown:
:description: To find the rank of a matrix with certain properties.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/b07a06d8-ea95-4798-9ae7-7d4bdea040de?id=70658
:label: grasple_exercise_4_2_19
:dropdown:
:description: To find the rank of a matrix with certain properties.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/547dd05d-c9da-4e7a-8deb-e4e8715e0f02?id=70660
:label: grasple_exercise_4_2_20
:dropdown:
:description: To find the dimension of the null space of a matrix with certain properties.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/e8051821-b278-4895-ad3e-bc40ba99e1dc?id=70659
:label: grasple_exercise_4_2_21
:dropdown:
:description: Does there exist a $3 \times 4$ matrix with dim Nul$(A) =$ dim Col$(A) = 2$?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/edb07654-fb52-4049-af08-e6333fd2d96e?id=83377
:label: grasple_exercise_4_2_22
:dropdown:
:description: Which three columns of a $4 \times 5$ matrix can be taken as a basis for its column space?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/af70726a-1610-4515-8f43-c11e109ca5cd?id=83396
:label: grasple_exercise_4_2_23
:dropdown:
:description: To give an example of a matrix $A$ with certain properties.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/3a957728-2365-4e80-820b-2d0f9dcf6ec3?id=83399
:label: grasple_exercise_4_2_24
:dropdown:
:description: To give an example of a matrix $A$ with certain properties.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/429fda86-ad72-402d-ab47-58ae103b36fc?id=83402
:label: grasple_exercise_4_2_25
:dropdown:
:description: Comparing the null spaces of two ($4\times 4$) matrices.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/608b1140-590f-468c-b704-eb922aa7fca4?id=83407
:label: grasple_exercise_4_2_26
:dropdown:
:description: Comparing the null spaces of two ($3\times 5$) matrices.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/d88ce866-54b2-48aa-bfe2-26116c2c6c34?id=83404
:label: grasple_exercise_4_2_27
:dropdown:
:description: Comparing the spans of two sets of vectors.

::::::
