(Sec:DetExtras)=

# Miscellaneous Applications of Determinants

In this section we will address the following matters:

<ul>

<li>

The determinant as a uniform scale factor for an arbitrary linear transformation from $\R^n$ to $\R^n$ .

</li>

<li>

Cramer's rule. Seemingly the ultimate solution to almost all systems of $n$ linear equations in $n$ unknowns.

</li>
<li>

The generalization of the formula

$$
\left[\begin{array}{cc} a &  b \\  c & d\end{array} \right]^{-1}
= \dfrac{1}{ad-bc}  \left[\begin{array}{cc} d & -b \\ -c & a\end{array}   \right]
$$

to $n\times n$ matrices.

</li>

<li>

A certain generalization of the cross product to $n$ dimensions.

</li>
</ul>

(SubSec:DetExtras:Volume+Orientation)=

## Volume and orientation revisited

We have seen in {numref}`Sec:DetGeometric` how determinants arise in the context of areas of parallelograms and volumes of parallelepipeds.

In {numref}`Sec:DotProduct` we used the dot product to define length, distance and orthogonality in $\R^n$. Determinants permit to define the concepts of volume and orientation in $n$ dimensions.

::::::{prf:definition} Theodorus in $\R^n$
:label: Dfn:DetExtras:VolumeRn

Let $\{\vect{v_1}, \ldots, \vect{v}_n\}$ be a set of $n$ vectors in $\R^n$.
The $n$-dimensional parallelepiped $\mathcal{P}$ spanned by $\vect{v_1}, \ldots, \vect{v}_n$ is the set

$$
\mathcal{P} = \{c_1\vect{v}_1+c_2\vect{v}_2 + \ldots + c_1\vect{v}_n \,|\, c_i \in \R,  0 \leq c_i \leq 1\}.
$$

See {numref}`Figure %s <Fig:DetExtras:Parped>` for an illustration of such a set in $\R^2$.

The **volume** of such a parallelepiped is defined by

$$
\text{Vol}(\mathcal{P}) = |\det{[\,\vect{v_1}\, \,\ldots \,\, \vect{v}_n\,]}|.
$$

So, it is the absolute value of a determinant.

::::::

Note that if the vectors $\vect{v}_1, \ldots, \vect{v}_n$ in {prf:ref}`Dfn:DetExtras:VolumeRn` are linearly dependent the volume automatically becomes 0.

::::::{figure} Images/Fig-DetExtras-ParPed.svg
:name: Fig:DetExtras:Parped
:class: dark-light

The parallelepiped generated by two vectors.
::::::

::::::{prf:definition} Orientation in $\R^n$
:label: Dfn:DetExtras:OrientationRn

Suppose the vectors $(\vect{v_1}, \ldots, \vect{v}_n)$ in $\R^n$ are linearly independent.

Then we say that the ordered set $(\vect{v_1}, \ldots, \vect{v}_n)$ is **positively oriented** &nbsp; if &nbsp; $ \det{[\vect{v_1} \ldots \vect{v}_n]}>0$.

If this determinant is negative the set is called **negatively oriented**.

For vectors that are linearly dependent we do not define the orientation.
::::::

::::::{prf:proposition}
:label: Prop:DetExras:DetAsScaleFactorR2

Suppose $T$ is a linear transformation from $\R^2$ to $\R^2$, with standard matrix $A = [\,\vect{a}_1 \,\,  \vect{a}_2\,]$. So we have

$$
T(\vect{x}) = A\vect{x}, \quad  \text{for} \,\, \vect{x} \,\text{  in  }\, \R^2.
$$

Let $R$ be any region in $\R^2$ for which the area is well-defined, and let $S$ be the image of $R$ under $T$.

Then for the area of $S$ it holds that

$$
 \text{area}(S) = |\det{A}|\cdot  \text{area}(R).
$$

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:DetExras:DetAsScaleFactorR2`
:class: tudproof

If the matrix $A$ is not invertible, the range of $T$, which is given by $\text{span}\{\vect{a}_1, \vect{a}_2\}$, is contained in a line.
Each region $R$ is then mapped onto a subset $S$ that is contained in this line, so

$$
  \text{area}(S) = 0 = 0\cdot \text{area}(R) =|\det{A}|\cdot \text{area}(R).
$$

Next suppose that $A$ is invertible. Then the unit grid is mapped onto a grid with as unit region the parallelogram with sides $\vect{a}_1 = A\vect{e}_1$ and $\vect{a}_2 = A\vect{e}_2$ . See {numref}`Figure %s <Fig:DetExtras:Grid>`.

::::{figure} Images/Fig-DetExtras-StandardGrid.svg
:name: Fig:DetExtras:Grid
:class: dark-light

The image of the standard grid.

::::

First we show that the formula holds if $R$ is the unit square, i.e., the parallelogram generated by $\vect{e}_1$ and $\vect{e}_2$. The unit square is mapped onto the parallelogram $S$ generated by $T(\vect{e}_1)=\vect{a}_1$ and $T(\vect{e}_2)=\vect{a}_2$. It follows
that

$$
\text{area}(S) = |\det{[\,\vect{a}_1\,\, \vect{a}_2\,]}|  = |\det{A}|,


$$

and since the area of $R$ is equal to 1, we have

$$
   |\det{A}| = |\det{A}| \cdot 1 =  |\det{A}| \cdot \text{area}(R).
$$

This then also holds for any square $R$ with sides of length $r$ that are parallel to the axes. Namely, such a square has area $r^2$ and can be described as the square with vertices

$$
\vect{p},\quad  \vect{p}+ r\vect{e}_1, \quad \vect{p}+ r\vect{e}_1+r\vect{e}_2 \quad\text{and}\quad  \vect{p}+ r\vect{e}_2.
$$

These are mapped to

$$
A\vect{p},\quad A\vect{p}+ rA\vect{e}_1, \quad A\vect{p}+ rA\vect{e}_1+rA\vect{e}_2 \quad\text{and}\quad  A\vect{p}+ rA\vect{e}_2.
$$

This is a parallelogram with sides $rA\vect{e}_1 = r\vect{a}_1$ and $rA\vect{e}_2 =r \vect{a}_2$, which has area

$$
\text{area}(S) =  |\det{[\,r\vect{a}_1\,\, r\vect{a}_2\,]}|   = r^2 |\det{A}| =   |\det{A}|\cdot \text{area}(R).
$$

See {numref}`Figure %s <Fig:DetExtras:ImageOfSquare>`

::::{figure} Images/Fig-DetExtras-ImageOfSquare.svg
:name: Fig:DetExtras:ImageOfSquare
:class: dark-light

The image of a square with 'corner' $\vect{p}$ and side length $r$.
::::

For a general (reasonable) region $R$ we sketch the idea and omit the technical details.

The region $R$ can be approximated arbitrarily close by a collection of smaller and smaller squares $R_i$ of which the interiors do not overlap.
See {numref}`Figure %s <Fig:DetExtras:Subdivision>`.
The limit of the areas of these approximations when the grids get finer and finer gives the area of $R$.

::::{figure} Images/Fig-DetExtras-Subdivision.svg
:name: Fig:DetExtras:Subdivision
:class: dark-light

Approximating a region by smaller and smaller squares.

::::

The formula holds for each of the $R_i$. Since $T$ is one-to-one, the images $S_i = T(R_i)$ will not overlap either, and the images taken together will approximate the image $S = T(R)$ as well. We deduce that

$$
  \begin{array}{rcl}
  \text{area}(S) \approx \sum \text{area}(S_i) &=& \sum  |\det{A}|\cdot \text{area}(R_i) \\
  &=& |\det{A}| \sum  \text{area}(R_i) \approx  |\det{A}|\cdot \text{area}(R).
  \end{array}
$$

By taking an appropriate limit one can show that in fact

$$
\text{area}(S) = |\det{A}|\cdot \text{area}(R).
$$

::::::

{prf:ref}`Prop:DetExtras:ScaleFactorR2` can be generalized to higher dimensions.  
For $n = 3$ area becomes volume, and for higher dimensions we use the definition of $n$-dimensional volume as in {prf:ref}`Dfn:DetExtras:VolumeRn`.

::::::{prf:proposition}
:label: Prop:DetExtras:ScaleFactorRn

Suppose $T$ is a linear transformation from $\R^n$ to $\R^n$, with standard matrix $A$.

Then for any region $S$ in $\R^n$ for which the volume is well-defined, it holds that

$$
 \text{Vol}(S) = |\det{A}|\cdot  \text{Vol}(R),
$$

where $S$ is the image of $R$ under $T$.

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:DetExtras:ScaleFactorRn`
:class: tudproof

<BR>

If $R$ is the $n$-dimensional parallelepiped $\mathcal{P}$ generated by $\{\vect{v}_1, \ldots, \vect{v}_n\}$ we have that $T(\mathcal{P})$ is generated by $\{T(\vect{v}_1), \ldots, T(\vect{v}_n)\}$.

Then

$$
  \begin{array}{rcl}
   \text{Vol}(T(\mathcal{P})) &=& |\det{[\,T(\vect{v_1})\, \,\ldots \,\, T(\vect{v}_n)\,]}| \\
   &=& |\det{[\,A(\vect{v_1})\, \,\ldots \,\, A(\vect{v}_n)\,]}| \\
   &=& |\det{\left(A [\,\vect{v_1}\, \,\ldots \,\, \vect{v}_n\,]\right)}|\\
   &=& |\det{A} \det{[\,\vect{v_1}\, \,\ldots \,\, \vect{v}_n\,]}|\\
   &=& |\det{A}| \,\text{Vol}(\mathcal{P}).
 \end{array}
$$

For a more general region $R$ we would again have to work with approximations/subdivisions like in the proof of {prf:ref}`Prop:DetExtras:ScaleFactorR2`. Then we would first have to extend the definition of $n$-dimensional volume. We will not pursue that track.

::::::

To conclude our interpretation of the determinant of $A$ regarding the linear transformation $T(\vect{x}) = A\vect{x}$ we look at the _orientation_.

::::{prf:proposition}
:label: Prop:DetExtras:ScaleFactorR2

Suppose $A = [\,\vect{a}_1\,\,\vect{a}_2\,\,\ldots\,\,\vect{a}_n\, ]$ is the standard matrix of the linear transformation $T: \R^n \to R^n$. So we have

$$
   T(\vect{x}) = A\vect{x}.
$$

Suppose $(\vect{v}_1,\,\vect{v}_2,\,\ldots\,,\,\vect{v}_n)$ is an ordered set of vectors in $\R^n$.

Then the following holds.

If $\det{A} > 0$,  the set $\big(T(\vect{v}_1),\,T(\vect{v}_2),\,\ldots\,,\,T(\vect{v}_n)\big)$ has the same orientation as the set $(\vect{v}_1,\,\vect{v}_2,\,\ldots\,,\,\vect{v}_n)$.

If $\det{A} < 0$ the set $\big(T(\vect{v}_1),\,T(\vect{v}_2),\,\ldots\,,\,T(\vect{v}_n)\big)$ has the opposite orientation as the set $(\vect{v}_1,\,\vect{v}_2,\,\ldots\,,\,\vect{v}_n)$.

In short: the transformation $T(\vect{x}) = A\vect{x}$ **preserves** the orientation if  
$\det{A} > 0$ and **reverses** the orientation if $\det{A} < 0$.

::::

If the determinant is 0, then the set $\{T(\vect{v}_1), \ldots,T(\vect{v}_n) \}$ will be linearly dependent, and for such a set the orientation is not defined.

::::{admonition} Proof of&nbsp;{prf:ref}`Prop:DetExtras:ScaleFactorR2`
:class: tudproof

This too follows immediately from the product rule of determinants.

$$
 \begin{array}{rcl}
 \det{\left[\,T(\vect{v}_1)\,\,T(\vect{v}_2)\,\,\ldots\,\,T(\vect{v}_n)\,  \right]} &=&
 \det{\left[\,A\vect{v}_1\,\,A\vect{v}_2\,\,\ldots\,\,A\vect{v}_n\,  \right]} \\
 &=&
 \det{\big(A\left[\,\vect{v}_1\,\,\vect{v}_2\,\,\ldots\,\,\vect{v}_n\,  \right]\big)} \\
 &=&
 \det{A}\cdot\det{\left[\,\vect{v}_1\,\,\vect{v}_2\,\,\ldots\,\,\vect{v}_n\,  \right]}.
 \end{array}
$$

::::

A nice illustration of what this means in $\R^2$ is given by the following example.

::::{prf:example}

Consider the two linear transformations from $\R^2$ to $\R^2$ with matrices

$$
  A = \begin{bmatrix} 1 & 3 \\ 3 & 1 \end{bmatrix}, \quad
  B = \begin{bmatrix} 3 & 1 \\ 1 & 3 \end{bmatrix}.
$$

Note that

$$
 \det{A} = -8 < 0 \quad \text{and} \quad \det{B} = 8 > 0.
$$

{numref}`Figure %s <Fig:DetExtras:Orientation>` visualizes what is going on.

```{applet}
:url: detextras/orientation
:fig: Images/Fig-DetExtras-Orientation.svg
:name: Fig:DetExtras:Orientation
:class: dark-light

Images under transformations with negative and positive determinant.
```

The images of a unit vector that rotates counterclockwise under transformation $A$ move around clockwise, i.e., in the _opposite_ orientation/direction. Under transformation $B$ the images will go around the origin counterclockwise, i.e., in the _same_ direction as the original vectors.

::::



(Subsec:DetExtra:Cramer)=

## Cramer's rule

We first introduce a new notation that will help to simplify formulas later.

::::::{prf:definition}
:label: Dfn:DetExtras:ReplaceColumn

Let $A$ be an $n\times n$ matrix, and $\vect{v}$ a vector in $\R^n$. Then $A^{(i)}(\vect{v})$ denotes the matrix that results when the $i$th column of $A$ is replaced by the vector $\vect{v}$.

::::::

::::::{prf:example}

For the matrix $A = \begin{bmatrix}  1 & 3 & 1 \\ 1 & 4 & 2 \\ 3 & 1 & 5  \end{bmatrix}$
and the vector $\vect{v} = \begin{bmatrix}  \class{blue}6 \\ \class{blue}7 \\ \class{blue}8 \end{bmatrix}$ we have that

$$
  A^{(2)}(\vect{v}) = \begin{bmatrix}  1 & \class{blue}6 & 1 \\ 1 & \class{blue}7 & 2 \\ 3 & \class{blue}8 & 5  \end{bmatrix}.
$$

::::::

Suppose that $A$ is an invertible $n \times n$ matrix. Then we know that the linear system $A\vect{x} = \vect{b}$ has a unique solution for each $\vect{b}$ in $\R^n$. And we also know that the determinant of $A$ is not equal to zero.
<BR>
The next proposition gives a ready-made formula for the solution.

::::::{prf:theorem} Cramer's Rule
:label: Thm:DetExtras:Cramer

Suppose $A$ is an invertible $n \times n$ matrix, and $\vect{b}$ a vector in $\R^n$.
The entries of $x_i$ of the unique solution $\vect{x}$ of the linear system

$$
A\vect{x} = \vect{b}
$$

are given by

:::::{math}
:label: Eq:DetExtras:Cramer

x_i = \dfrac{\det{\left(A^{(i)}(\vect{b})\right)}}{\det{A}}.

:::::

::::::

::::::{prf:example}

We use Cramer's rule to solve the system

$$
\left\lbrace
\begin{array}{rcc}
x_1  + 2x_2  + x_3  & = & 3 \\
x_1  - x_2   + 2x_3 & = & 4 \\
3x_1  + x_2   -5x_3  & = & 1
\end{array}
\right.
\quad\quad \text{i.e.,} \quad
\begin{bmatrix}
1 & 2 & 1 \\ 1 & -1 & 2 \\ 3 & 1 & -5
\end{bmatrix}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3  \end{array} \right]
 =
\begin{bmatrix}3 \\ 4 \\ 1  \end{bmatrix}.
$$

First of all, the determinant of $A$ can be computed as follows (in the first step we use column reduction, with the boxed 1 as a pivot):

$$
\left|\begin{array}{ccc} \fbox{$1$} & 2 & 1 \\ 1 & -1 & 2 \\ 3 & 1 & -5  \end{array} \right|=
\left|\begin{array}{ccc}  1 & 0 & 0 \\ 1 & -3 & 1 \\ 3 & -5 & -8  \end{array} \right|=
\left|\begin{array}{cc}   -3 & 1 \\  -5 & -8  \end{array} \right|= 29 \neq 0,
$$

so the coefficient matrix is invertible and consequently the system has a unique solution.

According to Cramer's rule we find the first entry of the solution as follows (again we use the boxed 1 as a pivot):

$$
x_1 = \dfrac{\begin{vmatrix}  3 & 2 & 1 \\ 4 & -1 & 2 \\ 1 & \fbox{$1$} & -5  \end{vmatrix}}{29} =
\dfrac{\begin{vmatrix}  1 & 0 & 11 \\ 5 & 0 & -3  \\ 1 & 1 & -5  \end{vmatrix}}{29} =
\dfrac{-\begin{vmatrix}  1 & 11 \\ 5 & -3   \end{vmatrix}}{29} = \dfrac{58}{29} = 2.
$$

Likewise we can compute the other two entries of the solution.

$$
x_2 = \dfrac{\begin{vmatrix}  1 & 3 & 1 \\ 1 & 4 & 2 \\ 3 & 1 & -5  \end{vmatrix}}{29} = 0
\quad \text{and} \quad
x_2 = \dfrac{\begin{vmatrix}  1 & 2 & 3 \\ 1 & -1 & 4 \\ 3 & 1 & 1  \end{vmatrix}}{29} = 1.
$$

::::::

The following proof of Cramer's rule rests rather nicely on properties of the determinant function. But feel free to skip it.

::::::{admonition} Proof of&nbsp;{prf:ref}`Thm:DetExtras:Cramer`
:class: tudproof, dropdown

Suppose $\vect{x} = \vect{c} = \left[\begin{array}{c}  c_1 \\ \vdots\\ c_n\end{array} \right]
$ is the unique solution of the linear system $A\vect{x} = \vect{b}$, with the invertible matrix $A = [ \vect{a}_1 \,  \,  \vect{a}_2 \, \ldots \,\vect{a}_n ]$.

We show that Formula {eq}`Eq:DetExtras:Cramer` holds for $c_1$. The argument can be copied for the other $c_i$.

We first note that

$$
\begin{array}{ccl}
A\vect{c} = \vect{b} &\iff \quad & c_1\vect{a}_1+c_2\vect{a}_2 + \ldots + c_n\vect{a}_n =\vect{b} \\
&\iff \quad & c_1\vect{a}_1+c_2\vect{a}_2 + \ldots + c_n\vect{a}_n - \vect{b} = \vect{0}.
\end{array}
$$

The smart next move is to replace the first column of $A$ by the zero column disguised as

$$
c_1\vect{a}_1+c_2\vect{a}_2 + \ldots + c_n\vect{a}_n - \vect{b}.
$$

So we have

$$
\det{[(c_1\vect{a}_1+ \,\ldots\, + c_n\vect{a}_n - \vect{b}) \,\,\,\vect{a}_2\,\, \ldots \,\, \vect{a}_n]}
=\det{[\vect{0} \,\,\vect{a}_2 \,\, \ldots\,\,\vect{a}_n]}  = 0.
$$

By the linearity property (in all of the columns) of the determinant ({prf:ref}`Prop:DetRowReduction:SumofCols`) we may deduce

:::{math}
:label: Eq:DetExtras:ProofCramer

c_1\det{(A)} + c_2\det{(A^{(1)}(\vect{a}_2))} + \ldots + c_n\det{(A^{(1)}(\vect{a}_n))} - \det{(A^{(1)}(\vect{b}))} = 0.

:::

Now we note that

$$
\det{A^{(1)}(\vect{a}_i)} = 0, \quad  i = 2,3,\ldots n,
$$

since in the matrix $A^{(1)}(\vect{a}_i)$ the first column and the $i$th column are identical. Hence all but the first and last determinant in Equation {eq}`Eq:DetExtras:ProofCramer` drop out and we can conclude that indeed

$$
c_1\det{(A)}  - \det{(A^{(1)}(\vect{b}))} = 0
\quad \iff \quad c_1 = \dfrac{\det{(A^{(1)}(\vect{b}))}}{\det{(A)}}.
$$

::::::

(Subsec:DetExtra:Inverse)=

::::::{prf:remark}
:label: Rem:DetExtras:Disclaimer1

Cramer's formula seems the solution to all your linear systems. However, it has its drawbacks.

<u>Disclaimer 1</u> &nbsp; Cramer's formula can only be used for a **square** linear system with an **invertible** matrix.

<u>Disclaimer 2</u> &nbsp; For a system with two equations in two unknowns Cramer's rule may come in handy, but for solving larger systems it is **highly inefficient**. For instance, for a system of four equations in four unknowns, to find the solution using Cramer's rule, one needs to compute five $4 \times 4$ determinants. The good-old method using the augmented matrix $[\,A\,|\,\vect{b}\,]$ only asks for one row reduction process.

::::::

## The inverse of a matrix in terms of determinants

As an interesting corollary of Cramer's Theorem we can give a ready-made formula for the inverse of an invertible matrix. The following proposition considers the notation of the previous section for a special case.

::::::{prf:proposition}
:label: Prop:DetExtras:ReplaceColGivesCofactor

Let $A$ be an $n\times n$ matrix, and $\vect{e}_j$ the $j$th vector of the standard basis of $\R^n$. Then

$$
\det{(A^{(i)}(\vect{e}_j))} = (-1)^{j+i} \det{A_{ji}} = C_{ji},
$$

where $A_{ji}$ is the submatrix and $C_{ji} =  (-1)^{j+i} \det{\left(A_{ji}\right)} $ the cofactor as introduced in the definition of the $n \times n$ determinant ({prf:ref}`Dfn:DetCofactors:Determinant`).

::::::

The following example serves as an illustration of what is going on here.

::::::{prf:example}

Let $A =  \left[\begin{array}{rrrr} 
a_{11} &a_{12} &a_{13} &a_{14}  \\ 
a_{21} &a_{22} &a_{23} &a_{24}  \\ 
a_{31} &a_{32} &a_{33} &a_{34}  \\ 
a_{41} &a_{42} &a_{43} &a_{44} 
\end{array} \right]
$ be any $4 \times 4$ matrix.

Then &nbsp;
$
A^{(4)}(\vect{e}_2) = \left[\begin{array}{rrrr}
a_{11} &a_{12} &a_{13} &0 \\
a_{21} &a_{22} &a_{23} &1 \\
a_{31} &a_{32} &a_{33} &0 \\
a_{41} &a_{42} &a_{43} &0
\end{array} \right].$

Expanding along the fourth column gives

$$
\det{(A^{(4)}(\vect{e}_2))} = \left|\begin{array}{rrrr}
a_{11} &a_{12} &a_{13} &0  \\
a_{21} &a_{22} &a_{23} &1  \\
a_{31} &a_{32} &a_{33} &0  \\
a_{41} &a_{42} &a_{43} &0
\end{array} \right|= (-1)^{(2+4)} \left|\begin{array}{rrr}
a_{11} &a_{12} &a_{13} \\
a_{31} &a_{32} &a_{33}  \\
a_{41} &a_{42} &a_{43}
\end{array} \right|= C_{24}.
$$

::::::

::::::{prf:proposition}
:label: Prop:DetExtras:Inverse

If $A$ is an invertible $n \times n$ matrix then the inverse $B$ of $A$ is given by

:::::{math}
:label: Eq:DetExtras:Inverse

B = \dfrac{1}{\det{A}} \left[\begin{array}{ccccc}
C_{11} &C_{21} &C_{31} & \ldots &C_{n1} \\
C_{12} &C_{22} &C_{32} & \ldots &C_{n2} \\
C_{13} &C_{23} &C_{33} & \ldots &C_{n3} \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
C_{1n} &C_{2n} &C_{3n} & \ldots &C_{nn} \\
\end{array} \right].
:::::

::::::

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:DetExtras:Inverse`
:class: tudproof

The $j$th column $\vect{b}_j$ of $B = A^{-1}$ is the solution of the linear system $A\vect{x} = \vect{e}_j$.

Cramer's rule then gives that $b_{ij}$, the $i$th entry of this column, is equal to

$$
b_{ij} = \dfrac{\det{\left(A^{(i)}(\vect{e}_j)\right)}}{\det{A}} = \dfrac{C_{ji}}{\det{A}}.
$$

For the last step we used {prf:ref}`Prop:DetExtras:ReplaceColGivesCofactor`.

::::::

::::::{prf:definition}

For an $n \times n$ matrix $A$ the matrix

$$
\left[\begin{array}{ccccc}
C_{11} &C_{12} &C_{13} & \ldots &C_{1n}  \\
C_{21} &C_{22} &C_{23} & \ldots &C_{2n}  \\
C_{31} &C_{32} &C_{33} & \ldots &C_{3n}  \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
C_{n1} &C_{n2} &C_{n3} & \ldots &C_{nn}  \\
\end{array} \right]
$$

is called its **cofactor matrix**.

The **adjugate matrix** of $A$ is defined as the transpose of the cofactor matrix. So

$$
\text{Adj}(A) =  \left[\begin{array}{ccccc}
C_{11} &C_{21} &C_{31} & \ldots &C_{n1}  \\
C_{12} &C_{22} &C_{32} & \ldots &C_{n2}  \\
C_{13} &C_{23} &C_{33} & \ldots &C_{n3}  \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
C_{1n} &C_{2n} &C_{3n} & \ldots &C_{nn}  \\
\end{array} \right]
.
$$

::::::

Thus {prf:ref}`Prop:DetExtras:Inverse` states that

$$
A^{-1} = \dfrac{1}{\det{A}} \text{Adj}(A),
$$

provided that $A$ is invertible.
In fact a slightly more general formula holds for **any** square matrix.

::::::{prf:proposition}
:label: Prop:DetExtras:AdjointProperty

For any square matrix $A$ the following identity holds:

$$
A\cdot\text{Adj}(A) = \text{Adj}(A)\cdot A = (\det{A})\cdot I.
$$

::::::

For clarity we used dots to indicate products. Note that the first two products are matrix products and the third product is a scalar times a matrix.

The proof we think, is short and instructive.

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:DetExtras:AdjointProperty`
:class: tudproof, dropdown

For an invertible matrix the statement follows immediately from {prf:ref}`Prop:DetExtras:Inverse`.

However, we can give an 'elementary' proof, that includes the non-invertible case where $\det{A}=0$. We will use two properties of determinants from earlier sections.
First {prf:ref}`Thm:DetCofactors:RowOrColumnExpansion`, that states that the determinant of a matrix can be found by expansion along an arbitrary column

$$
\det{A} = \sum_{i=1}^n   (-1)^{i+j} a_{ij}\det{A_{ij}} = \sum_{i=1}^n   a_{ij} C_{ij}.
$$

And second {prf:ref}`Cor:DetRowReduction:EqualRows`: the determinant of a matrix with two equal rows (or columns) is equal to 0.

Let us consider the product &nbsp; $\text{Adj}(A) \cdot A$ &nbsp; very carefully:

$$
\left[\begin{array}{ccccc}
C_{11} &C_{21} &C_{31} & \ldots &C_{n1}  \\
C_{12} &C_{22} &C_{32} & \ldots &C_{n2}  \\
C_{13} &C_{23} &C_{33} & \ldots &C_{n3}  \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
C_{1n} &C_{2n} &C_{3n} & \ldots &C_{nn}  \\
\end{array} \right]
\left[\begin{array}{ccccc}
a_{11} &a_{12} &a_{13} & \ldots &a_{1n}  \\
a_{21} &a_{22} &a_{23} & \ldots &a_{2n}  \\
a_{31} &a_{32} &a_{33} & \ldots &a_{3n}  \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
a_{n1} &a_{n2} &a_{n3} & \ldots &a_{nn}  \\
\end{array} \right].
$$

On the diagonal we see that the $j$th entry is equal to

$$
C_{1j}a_{1j} + C_{2j}a_{2j} + \ldots + C_{nj}a_{nj} = \sum_{i=1}^n   a_{ij} C_{ij} = \det{A}.
$$

For the off-diagonal elements we find as product of the $j$th row of $\text{Adj}(A)$ with the $k$th column of $A$ the sum

$$
C_{1j}a_{1k} + C_{2j}a_{2k} + \ldots + C_{nj}a_{nk} = \sum_{i=1}^n   a_{ik} C_{ij}.
$$

This expression can be interpreted as the expansion along the $k$th row of the determinant of the matrix $A^{(j)}(\vect{a}_k)$ that results if the $j$th column of
$A$ is replaced by the $k$th column of $A$. Since this matrix has two equal columns, its determinant must be zero!

::::::

For $n = 2$ {prf:ref}`Prop:DetExtras:Inverse` gives us back the old formula for the inverse.
That is, if we define the determinant of a $1 \times 1$ matrix $A = [a]$ as the number $a$ .

For an arbitrary invertible $3 \times 3$ matrix
$A=\left[\begin{array}{ccc} a_{11} &a_{12} &a_{13} \\ a_{21} &a_{22} &a_{23} \\ a_{31} &a_{32} &a_{33} \end{array} \right] $ the formula yields

$$
A^{-1}  =  \dfrac{1}{\begin{vmatrix}
a_{11} &a_{12} &a_{13}   \\
a_{21} &a_{22} &a_{23}  \\
a_{31} &a_{32} &a_{33}
\end{vmatrix}}
\left[\begin{array}{ccc}
\begin{vmatrix} a_{22} & a_{23} \\ a_{32} & a_{33}  \end{vmatrix} &
- \begin{vmatrix} a_{12} & a_{13} \\ a_{32} & a_{33}  \end{vmatrix} &
\begin{vmatrix} a_{12} & a_{13} \\ a_{22} & a_{23}  \end{vmatrix} \\
- \begin{vmatrix} a_{21} & a_{23} \\ a_{31} & a_{33}  \end{vmatrix} &
\begin{vmatrix} a_{11} & a_{13} \\ a_{31} & a_{33}  \end{vmatrix} &
- \begin{vmatrix} a_{21} & a_{31} \\ a_{22} & a_{32}  \end{vmatrix} \\
\begin{vmatrix} a_{21} & a_{22} \\ a_{31} & a_{32}  \end{vmatrix} &
- \begin{vmatrix} a_{11} & a_{31} \\ a_{12} & a_{32}  \end{vmatrix} &
\begin{vmatrix} a_{11} & a_{12} \\ a_{21} & a_{22}  \end{vmatrix}
\end{array}
\right].
$$

::::::{prf:remark}
:label: Rem:DetExtras:Disclaimer2

Like Cramer's rule, the formula for the inverse is **highly inefficient**.
The comparison between the efforts required to compute the inverse via the adjugate matrix versus row reduction of the augmented matrix $[\,A\,|\,I\,]$ works out rather favorably for the latter. A glimpse of this insuffiency is shown by the above formula for the inverse of a $3 \times 3$ matrix.

::::::

(Subsec-DetExtras-DetAndCrossProd)=

## Determinant and cross product

In {numref}`Sec:CrossProduct` the cross product of two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\R^3$ is defined. It is the unique vector $\mathbf{w}$ that is (1) orthogonal to $\mathbf{u}$ and $\mathbf{v}$, with (2) length equal to the area of the parallelogram with sides $\mathbf{u}$ and $\mathbf{v}$, and (3) such that the triple $(\mathbf{u},\mathbf{v},\mathbf{w})$ is 'righthanded' (= positively oriented).

In {numref}`Sec:DetGeometric`
we defined the determinant of the ordered set $(\vect{a},\vect{b},\vect{c})$ in $\R^3$ via

$$
\begin{array}{rcl}
\det{(\vect{a},\vect{b},\vect{c})} &=& (\vect{a}\times\vect{b})\ip\vect{c} =
\left|\begin{array}{ccc}  a_1 & b_1 &c_1 \\ a_2 & b_2 &c_2 \\ a_3 & b_3 & c_3   \end{array}\right|\\
&=&
\left|\begin{array}{cc}  a_2 & a_3   \\b_2 & b_3      \end{array}\right| c_1
- \left|\begin{array}{cc}  a_1 & a_3   \\b_1 & b_3      \end{array}\right| c_2
+ \left|\begin{array}{cc}  a_1 & a_2   \\b_1 & b_2      \end{array}\right| c_3.
\end{array}
$$

Conversely, we can write the cross product in terms containing determinants.

:::{math}
:label: Eq:DetExtras:DetCrossProd

\begin{array}{rcl}
\left[\begin{array}{c} a_1 \\ a_2 \\ a_3 \end{array}\right] \times
\left[\begin{array}{c}b_1 \\ b_2 \\ b_3 \end{array}\right]
&=& \left[\begin{array}{c}a_2b_3-a_3b_2 \\ a_3b_1 - a_1b_3 \\ a_2b_1-a_2b_1 \end{array}\right] \\
&=&
\left|\begin{array}{cc} a_2 & b_2 \\a_3 & b_3 \end{array}\right|\vect{e}_1
- \left|\begin{array}{cc} a_1 & b_1 \\ a_3 & b_3 \end{array}\right|\vect{e}_2
+ \left|\begin{array}{cc} a_1 & b_1 \\a_2 & b_2 \end{array}\right|\vect{e}_3.
  \end{array}

:::

The last expression can formally be written as

$$
\left|\begin{array}{ccc}  a_1 & b_1 &\vect{e}_1 \\ a_2 & b_2 &\vect{e}_2 \\ a_3 & b_3 & \vect{e}_3   \end{array}\right|.
$$

In exactly the same fashion, we can, for $n-1$ vectors $\vect{a}_1, \ldots, \vect{a}_{n-1}$ in $\R^n$, say

$$
\vect{a}_1 = \left[\begin{array}{c}  a_{11} \\ a_{21} \\ \vdots \\  a_{n1}      \end{array}\right], \quad
\vect{a}_2 = \left[\begin{array}{c}  a_{12} \\ a_{22} \\ \vdots \\  a_{n2}      \end{array}\right], \quad
\ldots \quad , \quad
\vect{a}_{n-1} = \left[\begin{array}{c}  a_{1,(n-1)} \\ a_{2(n-1)} \\ \vdots \\  a_{n,(n-1)}      \end{array}\right]
$$

define

:::{math}
:label: Eq:DetExtras:DetCrossProd-ndim

\vect{a}^{\ast}_n = \vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1}) =  \left|\begin{array}{ccccc}
a_{11} & a_{12} & \ldots & a_{1,(n-1)} & \vect{e}_1 \\
a_{21} & a_{22} & \ldots & a_{2,(n-1)} & \vect{e}_2 \\
\vdots & \vdots &        &    \vdots   &  \vdots \\
a_{n1} & a_{n2} & \ldots & a_{n,(n-1)} & \vect{e}_n
\end{array}\right|.

:::

Here $\vect{e}_1, \ldots , \vect{e}_n$ denote the vectors of the standard basis for $\R^n$.

With some effort it can be shown that the following properties hold.

::::::{prf:proposition}
:label: Prop:DetExtras:Properties-ndimCrossProd

Suppose that $\vect{a}_1, \ldots, \vect{a}_{n-1}$ are vectors in $\R^n$ and $\vect{a}^{\ast}_n$ is defined as in Equation {eq}`Eq:DetExtras:DetCrossProd-ndim`. Then the following properties hold.

<ol type = "i">
<li>

$\vect{a}^{\ast}_n \perp \vect{a}_i$, for $i = 1,2,\ldots, n-1$ .

</li>
<li>

$ \{\vect{a}_1, \, \ldots, \,\vect{a}_{n-1}\}$ is linearly dependent if and only if $\det{\left[\,\vect{a}_1\,\, \ldots\,\, \vect{a}_{n-1}\,\, \vect{a}^{\ast}_n\,\right]} = 0$.

</li>
<li>

If $ \{\vect{a}_1, \ldots, \vect{a}_{n-1}\}$ is linearly independent, then $\det{\left[\,\vect{a}_1, \ldots, \vect{a}_{n-1}, \vect{a}^{\ast}_n\,\right]
} > 0$.

</li>
<li>

The norm of the vector $\vect{a}^{\ast}_n$ is equal to the $(n-1)$-dimensional volume of the $(n-1)$-dimensional parallelepiped generated by $\vect{a}_1, \ldots, \vect{a}_{n-1}$.

</li>
</ol>

::::::

For an independent set of vectors $\{\vect{a}_1, \ldots, \vect{a}_{n-1}\}$ in $\R^n$, the properties of {prf:ref}`Prop:DetExtras:Properties-ndimCrossProd` uniquely determine $\vect{a}^{\ast}_n$
as the vector $\vect{v}$ that is orthogonal to $ \vect{a}_1, \ldots, \vect{a}_{n-1}$, has a prescribed length, and makes the  ordered set
$(\vect{a}_1, \ldots, \vect{a}_{n-1}, \vect{v}) $ positively oriented. <BR>
For a linearly dependent set of vectors property iv. implies that $\vect{a}^{\ast}_n = \vect{0}$.

::::::{prf:example}

For $n = 2$ we get, for an arbitrary vector $\vect{v} = 
\left[\begin{array}{c}   a \\ b   \end{array}\right]
 \neq \left[\begin{array}{c}   0\\0  \end{array}\right]
$:

$$
\vect{w} = \vect{N}\left(\vect{v}\right)
 =
\left|\begin{array}{cc}
a & \vect{e}_1\\
b & \vect{e}_2
\end{array}\right|=  a\vect{e}_1 - b\vect{e}_2 = \left[\begin{array}{c}    -b \\ a          \end{array}\right|.
$$

This is indeed a vector orthogonal to $\vect{v}$ with the same 'one-dimensional volume', i.e., length, as the vector $\vect{v}$.

Moreover, $\left(\vect{v}, \vect{w}\right)
 =  \left(\left[\begin{array}{c}   a \\ b   \end{array}\right]
,  \left[\begin{array}{c} -b \\ a   \end{array}\right]
 \right)
$
is positively oriented, as can be seen by making a sketch.

This shows that the construction also works in $\R^2$.

::::::

::::::{prf:example}

We will find the vector $\vect{n} = N(\vect{a}_1, \vect{a}_2, \vect{a}_3)$ for the columns of the matrix

$$
A = \left[\begin{array}{ccc}
                 1 &  1 &  3 \\
                 1 & -1 &  1 \\
                 1 &  1 & -3 \\
                -1 &  1 &  1
       \end{array}\right].
$$

The first entry $n_1$ is computed as

$$
 n_1 = (-1)^{1+4}\left|\begin{array}{ccc}
                  1 & -1 &  1 \\
                  1 &  1 & -3 \\
                 -1 &  1 &  1
        \end{array}\right| =
        -\left|\begin{array}{ccc}
                  1 & -1 &  1 \\
                  0 &  2 & -4 \\
                  0 &  0 &  2
        \end{array}\right| = -4.
$$

All in all we find

$$
  \vect{n} = \left[\begin{array}{c}
                  -4 \\ 12 \\ 4 \\ 12
        \end{array}\right] =  (-4)\left[\begin{array}{c}
                  1 \\ -3 \\ -1 \\ -3
        \end{array}\right].
$$

By taking inner products, or by computing $A^T\vect{n}$, it is checked that indeed
$\vect{n} \perp \vect{a}_i$ for each column $\vect{a}_i$. So property i. of {prf:ref}`Prop:DetExtras:Properties-ndimCrossProd` is satisfied.

Since the three columns are orthogonal, the 'rectangular box' in $\R^4$ they generate will have 3d-volume

$$
   \norm{\vect{a}_1} \cdot \norm{\vect{a}_2} \cdot \norm{\vect{a}_3} =
   \sqrt{4}\cdot \sqrt{4}\cdot  \sqrt{20} = 8\sqrt{5}.
$$

This is indeed equal to

$$
   \norm{\vect{n}} = \sqrt{4^2+12^2+4^2+12^2} = \sqrt{320},
$$

so property iv. is satisfied too.
::::::

We end the chapter with a proof of {prf:ref}`Prop:DetExtras:Properties-ndimCrossProd`.
<BR>
So, if you are interested, push the button on the right.

::::::{admonition} Proof of&nbsp;{prf:ref}`Prop:DetExtras:Properties-ndimCrossProd`
:class: tudproof, dropdown

The properties follow from the observation that for each vector $\vect{v}$ in $\R^n$

:::::{math}
:label: Eq:DetExtras:Deta1anv

\begin{array}{rcl}
\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})\ip\vect{v}
&=& \left|\begin{array}{ccccc}
a_{11} & a_{12} & \ldots & a_{1,(n-1)} & v_1 \\
a_{21} & a_{22} & \ldots & a_{2,(n-1)} & v_2 \\
\vdots & \vdots & & \vdots & \vdots \\
a_{n1} & a_{n2} & \ldots & a_{n,(n-1)} & v_n
\end{array}\right|\\
&=& \det{[\,\vect{a}_1\,\, \ldots\,\, \vect{a}_{n-1}\,\,\vect{v}\,]}.
\end{array}
:::::

This immediate generalization of the identity $(\vect{a}\times\vect{b})\ip\vect{c} = \det{[\,\vect{a}\,\,\vect{b}\,\,\vect{c}\,] }$ follows if
we write Equation {eq}`Eq:DetExtras:DetCrossProd-ndim` as in Equation {eq}`Eq:DetExtras:DetCrossProd`.

<ol type = "i">

<li>

Take any of the vectors $\vect{a}_j$ . Then (by Equation {eq}`Eq:DetExtras:Deta1anv`)

<BR>

$$
\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})  \ip \vect{a}_j= \det{ \left[\,\vect{a}_1\,\, \ldots\,\, \vect{a}_{n-1}\,\,\vect{a}_j\, \right]
 } = 0,
$$

since the determinant has two equal columns.
So indeed

<BR>

$$
\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})  \perp \vect{a}_j,\quad j = 1, \ldots, n-1.
$$

</li>
<li>

First suppose that the columns of the matrix

$$
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1,(n-1)} \\
a_{21} & a_{22} & \ldots & a_{2,(n-1)}  \\
\vdots & \vdots &        &      \vdots \\
a_{n1} & a_{n2} & \ldots & a_{n,(n-1)}
\end{bmatrix}
$$

are linearly dependent. Then for each vector $\vect{v}$ in $\R^n$

<BR>

$$
\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})  \ip \vect{v}  =  \det{ \left[\,\vect{a}_1\,\, \ldots\,\, \vect{a}_{n-1}\,\,\vect{v}\, \right]
 }  =  0.
$$

Namely, the first $n-1$ columns in the determinant are already linearly dependent.
This implies that $\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1}) $ must be the zero vector.

<BR>

Conversely, if the vectors $\{ \vect{a}_1, \,\ldots\, \, , \vect{a}_{n-1} \}$ are linearly independent,
then the $n \times (n-1)$ matrix $A = [ \,\vect{a}_1 \,\,  \ldots \,\,  \vect{a}_{n-1} \,] $  has rank  $n-1$ . The matrix $A$ must have $n-1$ linearly independent rows. Say, if we delete the $k$th row we have an $(n-1) \times (n-1)$ sub-matrix with independent rows.
Then the coefficient of $\vect{e}_k$ in the expansion of
$ \vect{N} ( \vect{a}_1, \ldots, \vect{a}_{n-1})$,
which by the defining Equation {eq}`Eq:DetExtras:DetCrossProd-ndim` is precisely (plus or minus) the determinant of this submatrix, is nonzero.

</li>
<li>

This is a consequence of the observation (again using {eq}`Eq:DetExtras:Deta1anv`)

<BR>

$$
\begin{array}{rcl}
  \det{\left[\,\vect{a}_1\,\, \ldots\,\, \vect{a}_{n-1}\,\,\vect{N}(\vect{a}_1, \ldots,  \vect{a}_{n-1})\, \right]}
  &=& \vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1}) \ip \vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})\\
  &=& \norm{\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})}^2 \geq 0,
\end{array}
$$

and the already established fact that $\vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1}) \neq \vect{0}$  if $\{\vect{a}_1,\, \ldots\,,\, \vect{a}_{n-1}\}$ is linearly independent.

</li>
<li>

We sketch the idea, which we borrow from volume versus area considerations in $\R^2$ and $\R^3$ .
We defined the volume of the $n$-dimensional parallelepiped $\mathcal{P} \left(\vect{a}_1, \ldots, \vect{a}_{n} \right) $  generated by the   $n$ vectors $\vect{a}_1, \ldots, \vect{a}_{n}$ as the absolute value of a determinant:

<BR>
 
$$ 
\text{Vol}_n\left(\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n}) \right)
 = |\det{\left[\,\vect{a}_1\,\, \ldots\,\, \,\vect{a}_{n}\,\right]
 }|. 
$$

The height times base principle in $\R^n$ must be:

if

$$
a_{n} \perp \mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n-1})
$$

then

<BR>

$$
\text{Vol}_n\left(\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n}) \right)
 =
\text{Vol}_{n-1} \left(\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n-1}) \right)
\cdot \norm{\vect{a}_{n}}.
$$

where $\text{Vol}_{n-1}$ denotes the $(n-1)$-dimensional volume of an $(n-1)$-dimensional subset of $\R^n$.

We apply this principle to the vector $\vect{a}_{n} = \vect{a}^{\ast}_n = \vect{N}(\vect{a}_1, \ldots, \vect{a}_{n-1})$.

We know that $\vect{a}^{\ast}_n$ is orthogonal to all vectors $\vect{a}_1, \ldots, \vect{a}_{n-1}$. So the 'height' of $\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n-1}, \vect{a}^{\ast}_n)$ is equal to $\norm{\vect{a}^{\ast}_n}$.

<BR>

On the one hand we then have that

<BR>

$$
\text{Vol}_n\!\left(\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n-1}, \vect{a}^{\ast}_n) \right)
 =
\text{Vol}_{n-1}\left(\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n-1}) \right)
\cdot \norm{\vect{a}^{\ast}_n}
$$

and on the other hand

<BR>

$$
\begin{array}{rcl}
\text{Vol}_n\!\left(\mathcal{P}(\vect{a}_1, \ldots, \vect{a}_{n-1}, \vect{a}^{\ast}_n) \right)
 &=&
|\det{ \left[\,\vect{a}_1\,\, \ldots\,\, \vect{a}_{n-1}\,\, \vect{a}^{\ast}_n\, \right]
 }| \\
&=&
| \vect{a}^{\ast}_n\ip \vect{a}^{\ast}_n| = \norm{\vect{a}^{\ast}_n}^2.
\end{array}.
$$

Equating the two expressions for $\text{Vol}_n \left(\mathcal{P} (\vect{a}\_1, \ldots, \vect{a}_{n-1}, \vect{a}^{\ast}_n) \right) $  
we conclude that indeed

<BR>

$$
\norm{\vect{a}^{\ast}_n} = \text{Vol}_{n-1} \left(\mathcal{P} (\vect{a}_1, \ldots, \vect{a}_{n-1}) \right).


$$

</li>
</ol>
 
::::::

## Grasple exercises

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/ddb8daf3-3773-44c9-8df0-fe3084a6e7c4?id=93170
:label: grasple_exercise_5_4_1
:dropdown:
:description: To compute the area of a triangle with sides $\vect{u}$ and $\vect{v}$ in the plane.
::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/8d7a0672-6283-4bb1-9b43-b41a03067e40?id=93171
:label: grasple_exercise_5_4_2
:dropdown:
:description: To find a point $C$ on a line, such that  the area of a triangle $ABC$ has a given value.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/62770bc4-da31-4212-a713-bb2843b0e580?id=93172
:label: grasple_exercise_5_4_3
:dropdown:
:description: Which points lie on the same side of a plane?

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/f787e084-9a77-40b4-b755-97890b98cfb6?id=93176
:label: grasple_exercise_5_4_4
:dropdown:
:description: To solve a 3x3 system using Cramer's rule.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/3add427a-88a3-4da0-8f0a-2bf8bb8781dd?id=93179
:label: grasple_exercise_5_4_5
:dropdown:
:description: Finding two entries in the inverse of a 4x4 matrix  (using the adjoint matrix).

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/8d4a98f7-50ac-4705-8b34-680b7b8395d9?id=93181
:label: grasple_exercise_5_4_6
:dropdown:
:description: To find a vector orthogonal to $\vect{v}_1,\vect{v}_2,\vect{v}_3$ in $\mathbb{R}^4$, with good orientation.

::::::

::::::{grasple}
:iframeclass: dark-light
:url: https://embed.grasple.com/exercises/bc3df113-95b3-470a-a730-3ad8faab08f5?id=93183
:label: grasple_exercise_5_4_7
:dropdown:
:description: To compute the normal vector $N(\vect{a}_1,\vect{a}_2,\vect{a}_3)$ as in {numref}`Subsection %s <Subsec-DetExtras-DetAndCrossProd>`.

::::::
